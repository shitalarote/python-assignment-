# -*- coding: utf-8 -*-
"""EDA Assignement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEn2JjliTW8zeFaYqCYDS_EAeaghPusV

# **EDA 1: Bike Details Dataset**


---
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set visualization style
sns.set(style="whitegrid")

# Download and load the dataset
url = "https://drive.google.com/uc?id=1iKy23bMtEQShF_weneRNnYrFmzvpPOI3"  # Modified Google Drive link
bike_data = pd.read_csv(url)

# Display the first few rows of the dataset
print(bike_data.head())

# Check basic dataset information
print(bike_data.info())



"""#**1. What is the range of selling prices in the dataset ?**"""

selling_price_min = bike_data['selling_price'].min()  # Minimum selling price
selling_price_max = bike_data['selling_price'].max()  # Maximum selling price

print(f"The range of selling prices is: {selling_price_min} to {selling_price_max}")

"""#**2. What is the median selling price for bikes in the dataset ?**"""

# Calculate the median selling price
median_selling_price = bike_data['selling_price'].median()

# Display the result
print(f"The median selling price for bikes is: {median_selling_price}")

"""#**3. What is the most common seller type?**"""

# Find the most common seller type
most_common_seller_type = bike_data['seller_type'].mode()[0]

# Display the result
print(f"The most common seller type is: {most_common_seller_type}")

"""#**4. How many bikes have driven more than 50,000 kilometers?**"""

# Filter bikes with km_driven > 50,000
bikes_over_50k = bike_data[bike_data['km_driven'] > 50000]

# Count the number of such bikes
count_bikes_over_50k = bikes_over_50k.shape[0]

# Display the result
print(f"The number of bikes that have driven more than 50,000 kilometers is: {count_bikes_over_50k}")

"""#**5. What is the average km_driven value for each ownership type?**"""

# Group data by ownership type and calculate the average km_driven
average_km_driven_by_owner = bike_data.groupby('owner')['km_driven'].mean()

# Display the result
print("Average km_driven for each ownership type:")
print(average_km_driven_by_owner)

"""#**6. What proportion of bikes are from the year 2015 or older?**"""

# Calculate the proportion of bikes from 2015 or older
proportion_2015_older = len(bike_data[bike_data['year'] <= 2015]) / len(bike_data)

# Display the result
print(f"The proportion of bikes from 2015 or older is: {proportion_2015_older}")

"""#**7. What is the trend of missing values across the dataset?**"""

# Check for missing values in each column
missing_values = bike_data.isnull().sum()

# Calculate the percentage of missing values in each column
missing_percentage = (missing_values / len(bike_data)) * 100

# Create a DataFrame to display the results
missing_data_summary = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})

# Sort the DataFrame by the percentage of missing values in descending order
missing_data_summary = missing_data_summary.sort_values('Percentage', ascending=False)

# Display the summary
print(missing_data_summary)


# Visualize the trend of missing values
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_data_summary.index, y=missing_data_summary['Percentage'])
plt.xlabel('Columns')
plt.ylabel('Percentage of Missing Values')
plt.title('Trend of Missing Values Across the Dataset')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.show()

"""#**8. What is the highest ex_showroom_price recorded, and for which bike?**"""

# Find the highest ex_showroom_price and the corresponding bike name
highest_ex_showroom_price = bike_data['ex_showroom_price'].max()
bike_with_highest_price = bike_data.loc[bike_data['ex_showroom_price'] == highest_ex_showroom_price, 'name']

print(f"The highest ex_showroom_price is: {highest_ex_showroom_price}")
print(f"The bike with the highest ex_showroom price is: {bike_with_highest_price.iloc[0]}")

"""#**9. What is the total number of bikes listed by each seller type?**"""

# Calculate the total number of bikes listed by each seller type
bike_counts_by_seller = bike_data.groupby('seller_type')['name'].count()

# Display the result
print("Total number of bikes listed by each seller type:")
bike_counts_by_seller

"""#**10. What is the relationship between selling_price and km_driven for first-owner bikes?**"""

# Filter for first-owner bikes
first_owner_bikes = bike_data[bike_data['owner'] == 'First Owner']

# Create a scatter plot to visualize the relationship
plt.figure(figsize=(10, 6))
sns.scatterplot(x='km_driven', y='selling_price', data=first_owner_bikes, alpha=0.5)
plt.xlabel('Kilometers Driven')
plt.ylabel('Selling Price')
plt.title('Relationship between Selling Price and Kilometers Driven for First-Owner Bikes')
plt.show()

# Calculate the correlation coefficient
correlation = first_owner_bikes['km_driven'].corr(first_owner_bikes['selling_price'])
print(f"The correlation between selling price and km_driven for first-owner bikes is: {correlation}")

"""#**11. Identify and remove outliers in the km_driven column using the IQR method?**"""

# Calculate Q1, Q3, and IQR
Q1 = bike_data['km_driven'].quantile(0.25)
Q3 = bike_data['km_driven'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify and remove outliers
bike_data_no_outliers = bike_data[~((bike_data['km_driven'] < lower_bound) | (bike_data['km_driven'] > upper_bound))]

# Display the cleaned dataset
bike_data_no_outliers

"""#**12. Perform a bivariate analysis to visualize the relationship between year and selling_price.**"""

# Create a scatter plot to visualize the relationship
plt.figure(figsize=(10, 6))
sns.scatterplot(x='year', y='selling_price', data=bike_data, alpha=0.5)
plt.xlabel('Year')
plt.ylabel('Selling Price')
plt.title('Relationship between Year and Selling Price')
plt.show()

# Calculate the correlation coefficient
correlation = bike_data['year'].corr(bike_data['selling_price'])
print(f"The correlation between year and selling_price is: {correlation}")

"""#**13. What is the average depreciation in selling price based on the bike's age (current year - manufacturing year).?**"""

# Calculate the age of each bike
current_year = 2024  # Assuming the current year is 2024
bike_data['bike_age'] = current_year - bike_data['year']

# Calculate the depreciation for each bike
bike_data['depreciation'] = bike_data['ex_showroom_price'] - bike_data['selling_price']

# Calculate the average depreciation per year of age
average_depreciation_per_year = bike_data.groupby('bike_age')['depreciation'].mean()

# Display the result
print("Average Depreciation per year of bike age:")
average_depreciation_per_year

"""#**14. Which bike names are priced significantly above the average price for their manufacturing year?**"""

# Calculate the average selling price for each year
average_price_by_year = bike_data.groupby('year')['selling_price'].mean()

# Merge the average price back into the original DataFrame
bike_data['average_price_for_year'] = bike_data['year'].map(average_price_by_year)

# Identify bikes priced significantly above average (e.g., 2 standard deviations above)
price_std = bike_data['selling_price'].std()
bike_data['price_diff'] = bike_data['selling_price'] - bike_data['average_price_for_year']

significantly_higher_priced_bikes = bike_data[bike_data['price_diff'] > 2*price_std]

# Display the names of these bikes
print("Bikes priced significantly above the average for their manufacturing year:")
print(significantly_higher_priced_bikes['name'])

"""#**15. Develop a correlation matrix for numeric columns and visualize it using a heatmap.**"""

# Calculate the correlation matrix for numeric columns
numeric_cols = bike_data.select_dtypes(include=np.number)
correlation_matrix = numeric_cols.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numeric Columns')
plt.show()

"""# **Car Sale Dataset**
---
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set visualization style
sns.set(style="whitegrid")

# Download and load the dataset
url = "https://drive.google.com/uc?id=1N0kM_zi-iU3yvr6BEJf21aiF599PtA0U" # Modified Google Drive link
car_data = pd.read_csv(url)

# Display the first few rows of the dataset
print(car_data.head())

# Check basic dataset information
print(car_data.info())

"""#**1. What is the average selling price of cars for each dealer, and how does it compare across different dealers?**"""

# Calculate the average selling price for each dealer
average_selling_price_by_dealer = car_data.groupby('Dealer_Name')['Price ($)'].mean()

# Display the result
print("Average Selling Price by Dealer:")
print(average_selling_price_by_dealer)

# Compare average selling prices across dealers (optional visualization)
plt.figure(figsize=(10, 6))
sns.barplot(x=average_selling_price_by_dealer.index, y=average_selling_price_by_dealer.values)
plt.xlabel('Dealer')
plt.ylabel('Average Selling Price')
plt.title('Comparison of Average Selling Prices Across Dealers')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

"""#**2. Which car brand (Company) has the highest variation in prices, and what does this tell us about the pricing trends?**"""

# Calculate the price variation (standard deviation) for each car brand
price_variation_by_brand = car_data.groupby('Company')['Price ($)'].std()

# Find the brand with the highest price variation
brand_with_highest_variation = price_variation_by_brand.idxmax()
highest_variation = price_variation_by_brand.max()

print(f"The car brand with the highest price variation is {brand_with_highest_variation} with a standard deviation of {highest_variation}.")

# Interpretation (example)
print(f"A high standard deviation in prices for {brand_with_highest_variation} suggests that their models span a wide range of price points, potentially indicating diverse features, model years, or conditions.")

"""#**3. What is the distribution of car prices for each transmission type, and how do the interquartile ranges compare?**"""

# Group data by transmission type and calculate relevant statistics
price_stats_by_transmission = car_data.groupby('Transmission')['Price ($)'].describe()

# Extract IQR for each transmission type
iqr_by_transmission = price_stats_by_transmission['75%'] - price_stats_by_transmission['25%']

# Display the results
print("Descriptive Statistics of Car Prices by Transmission Type:")
print(price_stats_by_transmission)
print("\nInterquartile Ranges (IQR) by Transmission Type:")
print(iqr_by_transmission)

# Visualization (optional)
plt.figure(figsize=(10, 6))
sns.boxplot(x='Transmission', y='Price ($)', data=car_data)
plt.xlabel('Transmission Type')
plt.ylabel('Price ($)')
plt.title('Distribution of Car Prices by Transmission Type')
plt.show()

"""#**4. What is the distribution of car prices across different regions?**"""

# Calculate the distribution of car prices across different regions
price_distribution_by_region = car_data.groupby('Dealer_Region')['Price ($)'].describe()

# Display the results
print("Distribution of Car Prices by Region:")
print(price_distribution_by_region)

# Visualization (optional)
plt.figure(figsize=(12, 6))
sns.boxplot(x='Dealer_Region', y='Price ($)', data=car_data)
plt.xlabel('Dealer_Region')
plt.ylabel('Price ($)')
plt.title('Distribution of Car Prices Across Different Regions')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

"""#**5. What is the distribution of cars based on body styles?**"""

# Calculate the distribution of cars based on body styles
body_style_counts = car_data['Body Style'].value_counts()

# Display the results
print("Distribution of Cars Based on Body Styles:")
print(body_style_counts)

# Visualization (optional)
plt.figure(figsize=(10, 6))
sns.countplot(x='Body Style', data=car_data)
plt.xlabel('Body Style')
plt.ylabel('Count')
plt.title('Distribution of Cars Based on Body Styles')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

"""#**6. How does the average selling price of cars vary by customer gender and annual income?**"""

# Assuming 'Customer_Gender' and 'Annual_Income' columns exist in your DataFrame
# If not, replace with the correct column names from your dataset

# Group data by customer gender and annual income and calculate the average selling price
average_price_by_gender_income = car_data.groupby(['Gender', 'Annual Income'])['Price ($)'].mean().unstack()

# Display the result
print("Average Selling Price by Customer Gender and Annual Income:")
print(average_price_by_gender_income)

# Visualization (optional)
plt.figure(figsize=(12, 6))
sns.heatmap(average_price_by_gender_income, annot=True, fmt=".2f", cmap="YlGnBu")
plt.xlabel("Annual Income")
plt.ylabel("Gender")
plt.title("Average Selling Price by Customer Gender and Annual Income")
plt.show()

"""#**7. What is the distribution of car prices by region, and how does the number of cars sold vary by region?**"""

# Calculate the distribution of car prices by region
price_distribution_by_region = car_data.groupby('Dealer_Region')['Price ($)'].describe()

# Display the price distribution
print("Distribution of Car Prices by Region:")
print(price_distribution_by_region)

# Calculate the number of cars sold in each region
cars_sold_by_region = car_data.groupby('Dealer_Region')['Price ($)'].count()

# Display the number of cars sold by region
print("\nNumber of Cars Sold by Region:")
print(cars_sold_by_region)

# Visualization (optional): Combine price distribution and sales volume
plt.figure(figsize=(12, 6))

# Box plot for price distribution
plt.subplot(1, 2, 1)
sns.boxplot(x='Dealer_Region', y='Price ($)', data=car_data)
plt.xlabel('Region')
plt.ylabel('Price ($)')
plt.title('Price Distribution by Region')
plt.xticks(rotation=45, ha='right')

# Bar plot for sales volume
plt.subplot(1, 2, 2)
sns.barplot(x=cars_sold_by_region.index, y=cars_sold_by_region.values)
plt.xlabel('Region')
plt.ylabel('Number of Cars Sold')
plt.title('Number of Cars Sold by Region')
plt.xticks(rotation=45, ha='right')

plt.tight_layout() # Adjust layout to prevent overlapping
plt.show()

"""#**8. How does the average car price differ between cars with different engine sizes?**"""

# Calculate the average car price for each engine size
average_price_by_engine_size = car_data.groupby('Engine')['Price ($)'].mean()

# Display the result
print("Average Car Price by Engine Size:")
print(average_price_by_engine_size)

# Visualization (optional)
plt.figure(figsize=(10, 6))
sns.barplot(x=average_price_by_engine_size.index, y=average_price_by_engine_size.values)
plt.xlabel('Engine')
plt.ylabel('Average Price ($)')
plt.title('Average Car Price by Engine Size')
plt.show()

"""#**9. How do car prices vary based on the customerâ€™s annual income bracket?**"""

# Create income brackets (adjust these as needed)
income_brackets = pd.cut(car_data['Annual Income'], bins=[0, 50000, 100000, 150000, float('inf')], labels=['0-50k', '50k-100k', '100k-150k', '150k+'])
car_data['Income Bracket'] = income_brackets

# Group by income bracket and calculate average car price
average_price_by_income_bracket = car_data.groupby('Income Bracket')['Price ($)'].mean()

# Display the result
print("Average Car Price by Annual Income Bracket:")
print(average_price_by_income_bracket)

# Visualization (optional)
plt.figure(figsize=(10, 6))
sns.barplot(x=average_price_by_income_bracket.index, y=average_price_by_income_bracket.values)
plt.xlabel('Annual Income Bracket')
plt.ylabel('Average Price ($)')
plt.title('Car Prices by Annual Income Bracket')
plt.show()

"""#**10. What are the top 5 car models with the highest number of sales, and how does their price distribution look?**"""

# Calculate the sales count for each car model
top_models = car_data['Model'].value_counts().nlargest(5)

print("Top 5 car models with the highest number of sales:")
print(top_models)

# Create subplots for price distributions of top models
plt.figure(figsize=(15, 10))

for i, model in enumerate(top_models.index):
    plt.subplot(2, 3, i + 1)
    sns.histplot(car_data[car_data['Model'] == model]['Price ($)'], kde=True)
    plt.title(f'Price Distribution for {model}')
    plt.xlabel('Price ($)')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""#**11. How does car price vary with engine size across different car colors, and which colors have the highest price variation?**"""

# Group data by engine size and color, then calculate the average price
price_variation_by_engine_color = car_data.groupby(['Engine', 'Color'])['Price ($)'].agg(['mean', 'std'])

# Find colors with the highest price variation for each engine size
max_variation_by_engine = price_variation_by_engine_color.groupby('Engine')['std'].idxmax()

print("Colors with the highest price variation for each engine size:")
print(max_variation_by_engine)

# Visualization (optional)
plt.figure(figsize=(12, 6))
for engine in price_variation_by_engine_color.index.get_level_values('Engine').unique():
    engine_data = price_variation_by_engine_color.loc[engine]
    sns.lineplot(x=engine_data.index, y=engine_data['mean'], label=engine)

plt.xlabel('Color')
plt.ylabel('Average Price ($)')
plt.title('Car Price Variation by Engine Size and Color')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Engine Size')
plt.show()

"""#**12. Is there any seasonal trend in car sales based on the date of sale?**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# If not, convert it first:
car_data['Date'] = pd.to_datetime(car_data['Date'])

# Extract month from the date
car_data['Month'] = car_data['Date'].dt.month

# Group by month and count sales
monthly_sales = car_data.groupby('Month')['Price ($)'].count()

# Visualize monthly sales
plt.figure(figsize=(10, 6))
sns.lineplot(x=monthly_sales.index, y=monthly_sales.values)
plt.xlabel('Month')
plt.ylabel('Number of Cars Sold')
plt.title('Seasonal Trend in Car Sales')
plt.xticks(range(1, 13))  # Set x-axis ticks to months
plt.show()

"""#**13. How does the car price distribution change when considering different combinations of body style and transmission type?**"""

# Group data by body style and transmission type, then calculate price distribution statistics
price_distribution_by_body_transmission = car_data.groupby(['Body Style', 'Transmission'])['Price ($)'].describe()

# Display the results
print("Car Price Distribution by Body Style and Transmission Type:")
print(price_distribution_by_body_transmission)

# Visualization (optional)
plt.figure(figsize=(12, 6))
sns.boxplot(x='Body Style', y='Price ($)', hue='Transmission', data=car_data)
plt.xlabel('Body Style')
plt.ylabel('Price ($)')
plt.title('Car Price Distribution by Body Style and Transmission Type')
plt.xticks(rotation=45, ha='right')
plt.show()

"""#**14. What is the correlation between car price, engine size, and annual income of customers, and how do these features interact?**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
# Convert 'Engine' to a numerical representation
car_data['Engine_Size'] = le.fit_transform(car_data['Engine'])
# Select relevant columns
relevant_data = car_data[['Price ($)','Engine_Size', 'Annual Income']]

# Calculate the correlation matrix
correlation_matrix = relevant_data.corr()

# Display the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation between Price, Engine Size, and Annual Income')
plt.show()

# Explore interactions using scatter plots
sns.pairplot(relevant_data)
plt.suptitle('Pairwise Relationships between Price, Engine Size, and Annual Income', y=1.02)
plt.show()

"""#**15. How does the average car price vary across different car models and engine types?**"""

# Calculate the average car price for each combination of car model and engine type
average_price_by_model_engine = car_data.groupby(['Model', 'Engine'])['Price ($)'].mean().unstack()

# Display the result
print("Average Car Price by Model and Engine Type:")
print(average_price_by_model_engine)

# Visualization (optional) - Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(average_price_by_model_engine, annot=True, fmt=".2f", cmap="viridis")
plt.xlabel("Engine Type")
plt.ylabel("Car Model")
plt.title("Average Car Price by Model and Engine Type")
plt.show()

"""#**EDA - 3 : Amazon Sales Data**
---
"""

# Load Amazon Sales Data
amazon_data_url = "https://drive.google.com/uc?id=1yKlW0x5yyRQijmWbEg5YCsq4wqLl3_oB"
amazon_data = pd.read_csv(amazon_data_url)

# Display dataset info
amazon_data.info()
amazon_data.head()

"""#**1.What is the average rating for each product category?**"""

# Calculate the average rating for each product category

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
# Convert 'rating' to a numerical representation
amazon_data['rating'] = le.fit_transform(amazon_data['rating'])

avg_rating_per_category = amazon_data.groupby('category')['rating'].mean()

# Display the result
print("Average Rating for Each Product Category:")
avg_rating_per_category

"""#**2. What are the top rating_count products by category?**"""

# Assuming 'rating_count' is a column in your amazon_data DataFrame.
# If not, replace 'rating_count' with the actual column name.
le = LabelEncoder()
# Convert 'rating' to a numerical representation
amazon_data['rating_count'] = le.fit_transform(amazon_data['rating_count'])

top_products_by_category = amazon_data.groupby('category').apply(lambda x: x.nlargest(1, 'rating_count'))

print("Top rating_count products by category:")
print(top_products_by_category[['product_name', 'rating_count']])

"""#**3. What is the distribution of discounted prices vs. actual prices?**"""

# Assuming 'actual_price' and 'discounted_price' are columns in your amazon_data DataFrame.
# If not, replace with the actual column names.

# Convert price columns to numeric, handling potential errors
amazon_data['actual_price'] = pd.to_numeric(amazon_data['actual_price'], errors='coerce')
amazon_data['discounted_price'] = pd.to_numeric(amazon_data['discounted_price'], errors='coerce')

# Drop rows with invalid price values
amazon_data.dropna(subset=['actual_price', 'discounted_price'], inplace=True)

# Calculate the difference between actual and discounted prices
amazon_data['price_difference'] = amazon_data['actual_price'] - amazon_data['discounted_price']


# Visualize the distribution using a histogram or a boxplot
plt.figure(figsize=(10, 6))
sns.histplot(amazon_data['price_difference'], kde=True)
plt.xlabel('Price Difference (Actual - Discounted)')
plt.ylabel('Frequency')
plt.title('Distribution of Discounted Prices vs. Actual Prices')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=amazon_data['price_difference'])
plt.xlabel('Price Difference (Actual - Discounted)')
plt.title('Distribution of Discounted Prices vs. Actual Prices (Boxplot)')
plt.show()

# Calculate descriptive statistics
price_difference_stats = amazon_data['price_difference'].describe()
print("Descriptive statistics of price differences:")
price_difference_stats

"""#**4. How does the average discount percentage vary across categories?**"""

# Calculate the discount percentage
amazon_data['discount_percentage'] = ((amazon_data['actual_price'] - amazon_data['discounted_price']) / amazon_data['actual_price']) * 100

# Group by category and calculate the average discount percentage
average_discount_by_category = amazon_data.groupby('category')['discount_percentage'].mean()

# Display the result
print("Average Discount Percentage by Category:")
print(average_discount_by_category)

# Visualization (optional)
plt.figure(figsize=(12, 6))
sns.barplot(x=average_discount_by_category.index, y=average_discount_by_category.values)
plt.xlabel('Category')
plt.ylabel('Average Discount Percentage')
plt.title('Average Discount Percentage Across Categories')
plt.xticks(rotation=45, ha='right')
plt.show()

"""#**5. What are the most popular product names?**"""

# Calculate the frequency of each product name
product_name_counts = amazon_data['product_name'].value_counts()

# Display the most popular product names (e.g., top 10)
print("Most Popular Product Names:")
print(product_name_counts.head(10))

# Visualization (optional) - Bar plot of top 10 product names
plt.figure(figsize=(12, 6))
sns.barplot(x=product_name_counts.head(10).index, y=product_name_counts.head(10).values)
plt.xlabel('Product Name')
plt.ylabel('Frequency')
plt.title('Top 10 Most Popular Product Names')
plt.xticks(rotation=45, ha='right')
plt.show()

"""#**6. What are the most popular product keywords?**"""

# Assuming 'product_name' contains keywords, you can extract them and count their frequency.
from collections import Counter

def extract_keywords(product_name):
    # Basic keyword extraction (you can refine this based on your needs)
    keywords = product_name.lower().split()  # Split the name into words
    return keywords

# Create a list to hold all keywords
all_keywords = []

for product_name in amazon_data['product_name']:
    keywords = extract_keywords(product_name)
    all_keywords.extend(keywords)

# Count the frequency of each keyword
keyword_counts = Counter(all_keywords)

# Get the most common keywords (e.g., top 10)
most_common_keywords = keyword_counts.most_common(10)

# Check if most_common_keywords is empty before unpacking
if most_common_keywords:
    print("Most Popular Product Keywords:")
    for keyword, count in most_common_keywords:
        print(f"{keyword}: {count}")

    # Visualization (optional) - Bar plot of top 10 keywords
    plt.figure(figsize=(12, 6))
    keywords, counts = zip(*most_common_keywords)  # Separate keywords and counts for plotting
    sns.barplot(x=list(keywords), y=list(counts))
    plt.xlabel('Keywords')
    plt.ylabel('Frequency')
    plt.title('Top 10 Most Popular Product Keywords')
    plt.xticks(rotation=45, ha='right')
    plt.show()
else:
    print("No common keywords found in product names.")

"""# **7. What are the most popular product reviews?**"""

# Assuming 'review_title' is a column in your amazon_data DataFrame.
# If not, replace 'review_title' with the actual column name.

popular_reviews = amazon_data['review_title'].value_counts()

print("Most Popular Product Review Titles:")
print(popular_reviews.head(10))  # Display the top 10 most popular review titles

# Visualization (optional) - Bar plot of top 10 review titles
plt.figure(figsize=(12, 6))
sns.barplot(x=popular_reviews.head(10).index, y=popular_reviews.head(10).values)
plt.xlabel('Review Title')
plt.ylabel('Frequency')
plt.title('Top 10 Most Popular Product Review Titles')
plt.xticks(rotation=45, ha='right')
plt.show()

"""#**8. What is the correlation between discounted_price and rating?**"""

# Calculate the correlation between 'discounted_price' and 'rating'
correlation = amazon_data['discounted_price'].corr(amazon_data['rating'])

print(f"The correlation between discounted_price and rating is: {correlation}")

"""#**9. What are the Top 5 categories based on the highest ratings?**"""

# Sort categories by average rating in descending order and get the top 5
top_5_categories = avg_rating_per_category.sort_values(ascending=False).head(5)

print("Top 5 categories based on the highest ratings:")
top_5_categories

"""#**10.  Identify any potential areas for improvement or optimization based on the data analysis.**

Ans:- Potential areas for improvement and optimization:

1. Data Cleaning and Preprocessing:
  - Handle missing values more effectively.  Instead of dropping rows with missing prices, consider imputation techniques (e.g., mean, median, or more sophisticated methods) to retain more data.
  - Investigate the meaning of '0' values in numerical columns (e.g. 'Annual Income', 'Price', 'Engine Size') and decide on the best way to handle them.
  - Inconsistent data formats may exist in some fields, such as dates or prices. Ensure consistency with type conversion and formatting.

2. Feature Engineering:
  - Create new features that capture important information not explicitly represented in the data. For example, derived features such as "Price per Horsepower," "Discount Percentage," or "Age of the Car" can improve analysis.
  - Explore creating interaction terms in the correlation analysis (e.g., 'Engine_Size' * 'Annual Income').
  - Binning numerical data ('Annual Income') to create meaningful categories may reveal additional insights.

3. Visualization Enhancements:
  - Use more informative plot types when appropriate.  For example, consider using scatter plots to examine the relationship between continuous variables instead of just heatmaps.
  - Improve the readability of visualizations by using clear labels, titles, and legends.
  - Explore more advanced visualizations like interactive plots for better exploration and communication of findings.

4. Statistical Analysis:
  - Perform more detailed statistical tests beyond correlation analysis, such as t-tests or ANOVA, to validate hypotheses about relationships between variables.
  - Conduct hypothesis testing to test assumptions before drawing conclusions.

5. Model Building (If applicable):
  - If you plan to build predictive models, carefully select appropriate models and features, and evaluate their performance rigorously.
  - Consider feature scaling and encoding when applying machine learning algorithms.
  - Optimize model hyperparameters for better accuracy and generalization.

6. Keyword Extraction Refinement:
  - Use more robust keyword extraction techniques beyond simple splitting.  Natural language processing (NLP) techniques can lead to better keyword identification.

7. Robustness of encoding:
  - LabelEncoder's application should be handled cautiously.  Explore OneHotEncoding or other encoding methods, especially when dealing with features that do not necessarily represent any ordinal relationship.

8.  Data Exploration Depth:
  - Dive deeper into the unique values of categorical columns and their distribution in relation to the target variable (e.g., 'Price').
  - Segment the data into relevant groups (e.g., by 'Region', 'Model') for more detailed analyses.

9. Documentation:
  - Include detailed explanations of the steps and rationale behind the data cleaning, feature engineering, and analysis. This improves code clarity and facilitates collaboration.

10. Code Structure:
  - Modularize the code into reusable functions for improved readability and maintainability.

By addressing these potential areas for improvement, you can gain a deeper understanding of the data, improve model accuracy (if applicable), and enhance the insights derived from the analysis.

#**EDA-04 Spotify Data**
---
"""

# Load Spotify Data
spotify_data_url = "https://drive.google.com/uc?id=1JVgy2yXUOREkzRdCeWjep626uvvF94pP"
spotify_data = pd.read_csv(spotify_data_url)

# Display dataset info
spotify_data.info()
spotify_data.head()

"""#**1. Read the dataframe, check null value if present then do the needful, check duplicate row , if present then do the needful?**"""

# Check for null values
print(spotify_data.isnull().sum())

# Handle null values (example: fill with 0)
spotify_data.fillna(0, inplace=True)

# Check for duplicate rows
duplicate_rows = spotify_data[spotify_data.duplicated()]
print(f"Number of duplicate rows: {len(duplicate_rows)}")

# Handle duplicate rows (example: remove duplicates)
spotify_data.drop_duplicates(inplace=True)

"""#**2. What is the distribution of popularity among the tracks in the dataset? Visualize it using a histogram?**"""

# Calculate the frequency of each popularity value
popularity_counts = spotify_data['Popularity'].value_counts()

# Create the histogram
plt.figure(figsize=(10, 6))
plt.hist(spotify_data['Popularity'], bins=20, edgecolor='black')  # Changed 'popularity' to 'Popularity'
plt.xlabel('Popularity')
plt.ylabel('Number of Tracks')
plt.title('Distribution of Track Popularity')
plt.show()

"""#**3. Is there any relationship between the popularity and the duration of tracks? Explore this using a scatter plot?**"""

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(spotify_data['Popularity'], spotify_data['Duration (ms)'], alpha=0.5)
plt.xlabel('Popularity')
plt.ylabel('Duration (ms)')
plt.title('Relationship between Popularity and Duration of Tracks')
plt.show()

"""#**4. Which artist has the highest number of tracks in the dataset? Display the count of tracks for each artist using a countplot.**"""

# Calculate the number of tracks for each artist
artist_track_counts = spotify_data['Artist'].value_counts()

# Find the artist with the highest number of tracks
artist_with_highest_tracks = artist_track_counts.index[0]
highest_track_count = artist_track_counts.iloc[0]

print(f"The artist with the highest number of tracks is {artist_with_highest_tracks} with {highest_track_count} tracks.")

# Create the countplot
plt.figure(figsize=(12, 6))
sns.countplot(x='Artist', data=spotify_data, order=artist_track_counts.index)
plt.xlabel('Artist')
plt.ylabel('Number of Tracks')
plt.title('Number of Tracks per Artist')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.show()

"""#**5. What are the top 5 least popular tracks in the dataset? Provide the artist name and track name for each.**"""

# Sort the DataFrame by popularity in ascending order and get the top 5
least_popular_tracks = spotify_data.sort_values(by='Popularity').head(5)

# Display the artist name and track name for the least popular tracks
print("Top 5 Least Popular Tracks:")
print(least_popular_tracks[['Artist', 'Track Name']])

"""#**6. Among the top 5 most popular artists, which artist has the highest popularity on average? Calculate and display the average popularity for each artist.**"""

# Find the top 5 most popular artists
top_5_artists = spotify_data['Artist'].value_counts().head(5).index

# Calculate the average popularity for each of the top 5 artists
average_popularity_by_artist = {}
for artist in top_5_artists:
    artist_data = spotify_data[spotify_data['Artist'] == artist]
    average_popularity = artist_data['Popularity'].mean()
    average_popularity_by_artist[artist] = average_popularity

# Find the artist with the highest average popularity among the top 5
artist_with_highest_avg_popularity = max(average_popularity_by_artist, key=average_popularity_by_artist.get)
highest_average_popularity = average_popularity_by_artist[artist_with_highest_avg_popularity]

# Display the results
print("Average Popularity for Top 5 Artists:")
for artist, avg_popularity in average_popularity_by_artist.items():
    print(f"{artist}: {avg_popularity}")

print(f"\nThe artist with the highest average popularity among the top 5 is {artist_with_highest_avg_popularity} with an average popularity of {highest_average_popularity}")

"""#**7. For the top 5 most popular artists, what are their most popular tracks? List the track name for each artist.**"""

# Find the top 5 most popular artists
top_5_artists = spotify_data['Artist'].value_counts().head(5).index

# Iterate through the top 5 artists and find their most popular track
for artist in top_5_artists:
    artist_data = spotify_data[spotify_data['Artist'] == artist]
    most_popular_track = artist_data.loc[artist_data['Popularity'].idxmax()]
    print(f"Artist: {artist}, Most Popular Track: {most_popular_track['Track Name']}")

"""#**8. Visualize relationships between multiple numerical variables simultaneously using a pair plot.**"""

# Create a pair plot for numerical variables in the spotify_data DataFrame.
numerical_cols = spotify_data.select_dtypes(include=np.number).columns
sns.pairplot(spotify_data[numerical_cols])
plt.show()

"""#**9. Does the duration of tracks vary significantly across different artists? Explore this visually using a box plot or violin plot.**"""

# Create the box plot
plt.figure(figsize=(12, 6))
sns.boxplot(x='Artist', y='Duration (ms)', data=spotify_data)
plt.xlabel('Artist')
plt.ylabel('Duration (ms)')
plt.title('Duration of Tracks Across Different Artists (Box Plot)')
plt.xticks(rotation=90)
plt.show()

# Create the violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x='Artist', y='Duration (ms)', data=spotify_data)
plt.xlabel('Artist')
plt.ylabel('Duration (ms)')
plt.title('Duration of Tracks Across Different Artists (Violin Plot)')
plt.xticks(rotation=90)
plt.show()

"""#**10. How does the distribution of track popularity vary for different artists? Visualize this using a swarm plot or a violin plot.**"""

# Create the swarm plot
plt.figure(figsize=(12, 6))
sns.swarmplot(x='Artist', y='Popularity', data=spotify_data)
plt.xlabel('Artist')
plt.ylabel('Popularity')
plt.title('Distribution of Track Popularity Across Different Artists (Swarm Plot)')
plt.xticks(rotation=90)
plt.show()

# Create the violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x='Artist', y='Popularity', data=spotify_data)
plt.xlabel('Artist')
plt.ylabel('Popularity')
plt.title('Distribution of Track Popularity Across Different Artists (Violin Plot)')
plt.xticks(rotation=90)
plt.show()