# -*- coding: utf-8 -*-
"""Statistics Advance 2  Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14In1OtSbLydJh3GKGN-yzqIKaFZqcmbS

# **Statistics Advance 2 Assignment 2**

---

**Q1.Generate a list of 100 integers containing values between 90 to 130 and store it in the variable `int_list'**
**After generating the list, find the following:**
"""

import random

# Generating a list of 100 integers between 90 and 130
int_list = [random.randint(90, 130) for _ in range(100)]
int_list

[120, 118, 91, 130, 108, 129, 110, 114, 108, 107, 95, 124, 90, 97, 97, 107, 115, 117, 103, 98, ...]

""" (i) Write a Python function to calculate the mean of a given list of numbers.

Create a function to find the median of a list of numbers.
"""

from typing import List

# Function to calculate the mean of a list of numbers
def calculate_mean(numbers: List[float]) -> float:
    return sum(numbers) / len(numbers) if numbers else 0

# Function to calculate the median of a list of numbers
def calculate_median(numbers: List[float]) -> float:
    sorted_numbers = sorted(numbers)
    n = len(sorted_numbers)
    if n == 0:
        return 0  # Return 0 if the list is empty
    midpoint = n // 2
    if n % 2 == 1:
        # If odd, return the middle number
        return sorted_numbers[midpoint]
    else:
        # If even, return the average of the two middle numbers
        return (sorted_numbers[midpoint - 1] + sorted_numbers[midpoint]) / 2

# Testing the functions with int_list
mean_value = calculate_mean(int_list)
median_value = calculate_median(int_list)

mean_value, median_value

"""(ii) Develop a program to compute the mode of a list of integers"""

from collections import Counter
from typing import List, Union

# Function to calculate the mode of a list of integers
def calculate_mode(numbers: List[int]) -> Union[int, List[int], None]:
    if not numbers:
        return None  # Return None if the list is empty

    frequency_counts = Counter(numbers)
    max_frequency = max(frequency_counts.values())

    # Find all numbers that have the max frequency
    mode_values = [num for num, freq in frequency_counts.items() if freq == max_frequency]

    # Return the mode(s) - if there's only one mode, return it as an integer; otherwise, return the list
    return mode_values[0] if len(mode_values) == 1 else mode_values

# Testing the mode function with int_list
mode_value = calculate_mode(int_list)

mode_value

"""(iii) Implement a function to calculate the weighted mean of a list of values and their corresponding weights."""

from typing import List

# Function to calculate the weighted mean of values with corresponding weights
def calculate_weighted_mean(values: List[float], weights: List[float]) -> float:
    if len(values) != len(weights) or len(values) == 0:
        raise ValueError("Values and weights must be of the same non-zero length")

    weighted_sum = sum(value * weight for value, weight in zip(values, weights))
    total_weight = sum(weights)

    # Calculate and return the weighted mean
    return weighted_sum / total_weight if total_weight != 0 else 0

# Example test values
values = [90, 100, 110, 120]
weights = [1, 2, 3, 4]

# Calculate the weighted mean for the example
weighted_mean_value = calculate_weighted_mean(values, weights)

weighted_mean_value

""" (iv) Write a Python function to find the geometric mean of a list of positive numbers."""

import math
from typing import List

# Function to calculate the geometric mean of a list of positive numbers
def calculate_geometric_mean(numbers: List[float]) -> float:
    if not numbers or any(num <= 0 for num in numbers):
        raise ValueError("All numbers must be positive and the list must be non-empty")

    product = math.prod(numbers)
    n = len(numbers)

    # Calculate the geometric mean
    return product ** (1 / n)

# Example list to test the function
example_numbers = [1.5, 2.0, 3.0, 4.0]

# Calculating the geometric mean of the example list
geometric_mean_value = calculate_geometric_mean(example_numbers)

geometric_mean_value

"""(v) Create a program to calculate the harmonic mean of a list of values."""

from typing import List

# Function to calculate the harmonic mean of a list of values
def calculate_harmonic_mean(numbers: List[float]) -> float:
    if not numbers or any(num <= 0 for num in numbers):
        raise ValueError("All numbers must be positive and the list must be non-empty")

    n = len(numbers)
    reciprocal_sum = sum(1 / num for num in numbers)

    # Calculate the harmonic mean
    return n / reciprocal_sum

# Example list to test the function
example_numbers = [1.5, 2.0, 3.0, 4.0]

# Calculating the harmonic mean of the example list
harmonic_mean_value = calculate_harmonic_mean(example_numbers)

harmonic_mean_value

"""(vi) Build a function to determine the midrange of a list of numbers (average of the minimum and maximum)."""

from typing import List

# Function to calculate the midrange of a list of numbers
def calculate_midrange(numbers: List[float]) -> float:
    if not numbers:
        raise ValueError("The list must be non-empty")

    min_value = min(numbers)
    max_value = max(numbers)

    # Calculate the midrange
    return (min_value + max_value) / 2

# Example list to test the function
example_numbers = [1.5, 2.0, 3.0, 4.0]

# Calculating the midrange of the example list
midrange_value = calculate_midrange(example_numbers)

midrange_value

"""(vii) Implement a Python program to find the trimmed mean of a list, excluding a certain percentage of
outliers.
"""

from typing import List
import statistics

def calculate_trimmed_mean(numbers: List[float], percentage: float) -> float:
    if not 0 <= percentage < 0.5:
        raise ValueError("Percentage must be between 0 and 0.5 (50%)")
    if not numbers:
        raise ValueError("The list must be non-empty")

    # Sort the list to trim the outliers
    sorted_numbers = sorted(numbers)
    n = len(sorted_numbers)

    # Calculate the number of elements to trim from each end
    trim_count = int(n * percentage)

    # Trim the sorted list
    trimmed_numbers = sorted_numbers[trim_count:n - trim_count]

    # Calculate the mean of the trimmed list
    return statistics.mean(trimmed_numbers)

# Example usage:
example_numbers = [10, 20, 30, 40, 50, 100, 150]
trimmed_mean_value = calculate_trimmed_mean(example_numbers, 0.1)  # Excludes 10% from each end
print("Trimmed Mean:", trimmed_mean_value)

"""**Q2.Generate a list of 500 integers containing values between 200 to 300 and store it in the variable `int_list2`.**
**After generating the list, find the following**
"""

# Generating a list of 500 integers between 200 and 300
int_list2 = [random.randint(200, 300) for _ in range(500)]
int_list2[:10]  # Displaying the first 10 elements as a preview

"""(i) Compare the given list of visualization for the given data:

 1. Frequency & Gaussian distribution

 2. Frequency smoothened KDE plot

3. Gaussian distribution & smoothened KDE plot

1. Frequency & Gaussian distribution

**Ans.** To compare the visualization of a **Frequency Distribution** and a **Gaussian Distribution**, let's break down the characteristics of each:

### 1. **Frequency Distribution**
- **Definition**: A frequency distribution is a way to represent how often each value or range of values appears in a dataset. It is often visualized using histograms or bar charts.
- **Purpose**: It shows the distribution of data across different intervals or categories.
- **Visualization Characteristics**:
  - **Shape**: It can take various forms, depending on the dataset. The shape is not predetermined and can be skewed, uniform, bimodal, etc.
  - **Axes**:
    - **X-axis**: Represents the data intervals (bins) or specific categories.
    - **Y-axis**: Represents the frequency or count of data points within each bin or category.
  - **Appearance**: A histogram with bars that represent the frequency of data in each interval.

#### Example:
If you have data on the age of a group of people, a frequency distribution might show how many people fall into different age ranges (e.g., 0-10, 11-20, etc.).

### 2. **Gaussian Distribution (Normal Distribution)**
- **Definition**: A Gaussian distribution, also known as the normal distribution, is a continuous probability distribution that is symmetric about the mean, with a bell-shaped curve.
- **Purpose**: It shows how data points are distributed around a central value (the mean) and is often used to model naturally occurring data, like height or test scores.
- **Visualization Characteristics**:
  - **Shape**: Symmetrical bell-shaped curve.
  - **Axes**:
    - **X-axis**: Represents the values of the data (e.g., test scores, height, etc.).
    - **Y-axis**: Represents the probability density, showing how likely data points are to occur near specific values.
  - **Appearance**: A smooth curve with the highest point at the mean, tapering off symmetrically toward both ends.
  
#### Example:
The heights of adult humans generally follow a Gaussian distribution, where most people are close to the average height, with fewer people being extremely tall or short.

### **Comparison**
- **Shape**:
  - **Frequency Distribution**: The shape is data-dependent and can be uneven (skewed, bimodal, etc.).
  - **Gaussian Distribution**: Always forms a bell curve, symmetric around the mean.
- **Interpretation**:
  - **Frequency Distribution**: Focuses on how often each value occurs. It is useful for understanding the distribution across intervals.
  - **Gaussian Distribution**: Provides insights into the probability of data points being close to the mean. It assumes a normal spread of data.
- **Application**:
  - **Frequency Distribution**: Good for both continuous and discrete data. It helps visualize raw data and distributions.
  - **Gaussian Distribution**: More relevant for modeling continuous data that follows a normal distribution, often used in statistical analysis.

### When to Use Each:
- **Frequency Distribution** is used when you have a dataset and want to visualize its distribution and how frequently each value occurs.
- **Gaussian Distribution** is used when you assume or want to test if the data follows a normal distribution. This is helpful in statistics for hypothesis testing, probability estimation, and data modeling.

2. Frequency smoothened KDE plot
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Example dataset
data = [23, 45, 56, 67, 45, 56, 78, 45, 34, 22, 56, 67, 89, 45]

# Create a KDE plot
sns.kdeplot(data, shade=True)

# Show the plot
plt.show()

"""3. Gaussian distribution & smoothened KDE plot

**Ans.**### **Gaussian Distribution vs Smoothened KDE Plot**

Both the **Gaussian Distribution** (Normal Distribution) and a **Smoothened KDE (Kernel Density Estimate) Plot** are used to describe and visualize the distribution of continuous data. While they both serve a similar purpose, they are different in terms of their construction and what they represent.

#### 1. **Gaussian Distribution (Normal Distribution)**

- **Definition**: The **Gaussian Distribution** is a specific, well-known continuous probability distribution characterized by its bell-shaped curve. It is symmetric about its mean, with data points being more concentrated around the mean and less frequent as you move farther away.
  
- **Mathematical Equation**:
  \[
  f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
  \]
  Where:
  - \(\mu\) is the mean of the distribution (the peak of the curve),
  - \(\sigma\) is the standard deviation (which determines the spread of the curve).

- **Shape**: The Gaussian distribution is **always** bell-shaped and symmetric around the mean. Its peak represents the mean value, and the spread (width) of the curve is determined by the standard deviation.
  
- **Properties**:
  - The area under the curve equals 1 (representing the total probability).
  - The majority of data points fall within \(\pm 1 \sigma\) from the mean (68% of the data), \(\pm 2 \sigma\) (95%), and \(\pm 3 \sigma\) (99.7%).

- **Use Case**: It's used when you assume or know that your data follows a normal distribution. It is commonly applied in statistical tests, modeling, and predictions when the data is believed to be normally distributed.

#### 2. **Smoothened KDE Plot**

- **Definition**: A **Kernel Density Estimate (KDE)** plot is a non-parametric way to estimate the probability density function of a continuous random variable. Unlike the Gaussian distribution, which is a theoretical model with a fixed shape, a KDE plot is based on the actual data points and creates a smoothed curve to represent the data's underlying distribution.

- **How It Works**: In a KDE plot:
  - A kernel function (commonly Gaussian) is placed at each data point.
  - The kernels are summed up to create a smooth curve that approximates the data's density distribution.

- **Shape**: The shape of the KDE plot depends on the underlying data. Unlike the Gaussian distribution, which has a fixed form, the KDE plot can have various shapes: it may be unimodal, bimodal, skewed, or multi-modal depending on the data. The smoothness of the plot is controlled by the **bandwidth** parameter, which determines how much smoothing is applied to the data.

- **Properties**:
  - **Non-parametric**: KDE does not assume a specific distribution for the data (like the Gaussian distribution). It uses the data itself to estimate the density.
  - **Smoothness**: The bandwidth parameter controls how smooth the estimate is. A smaller bandwidth results in a more jagged curve, while a larger bandwidth gives a smoother curve.
  - **Flexibility**: The KDE can model distributions that are not Gaussian, such as bimodal distributions (with two peaks) or distributions with skewness.

- **Use Case**: KDE plots are useful for visualizing the actual distribution of a dataset without assuming a specific underlying distribution. It is ideal for exploratory data analysis, especially when you suspect the data might not follow a normal distribution.

### **Comparison**:

| Feature                    | **Gaussian Distribution**                          | **Smoothened KDE Plot**                         |
|----------------------------|---------------------------------------------------|------------------------------------------------|
| **Assumptions**             | Assumes a specific normal (Gaussian) distribution. | Non-parametric, based on the actual data.      |
| **Shape**                   | Fixed bell curve (symmetric).                     | Varies depending on the data, can be unimodal, bimodal, etc. |
| **Application**             | Used for data that is known or assumed to follow a normal distribution. | Used for exploring and estimating the distribution of any continuous data. |
| **Mathematical Formula**    | Defined by a fixed mathematical formula.          | Based on data points and kernels, with no fixed formula. |
| **Smoothness**              | Fixed smoothness determined by the standard deviation. | Smoothness controlled by bandwidth parameter. |
| **Interpretation**          | Represents the likelihood of a value occurring in a normal distribution. | Provides an estimate of the data's probability density function, which can have more complex shapes. |
| **Data Dependency**         | Does not depend on data, it is a theoretical model. | Directly depends on the data and its distribution. |

### **Visual Comparison**:
- A **Gaussian Distribution** will always look like a bell curve, symmetric about its mean.
- A **KDE plot**, on the other hand, can take on any shape, depending on the data. It will still smooth out the frequencies but doesn't assume any specific distribution like the Gaussian.

#### Example:

- **Gaussian Distribution**: If you have data on people's heights, and you know that the heights are normally distributed, the Gaussian distribution will give you a theoretical bell curve for the data.
  
- **KDE Plot**: If you don't know if the height data is normally distributed, a KDE plot will show you the actual distribution of the data, whether it is bell-shaped, skewed, or bimodal.

### **When to Use Each**:
- **Gaussian Distribution**: Use this when you know or assume your data follows a normal distribution (e.g., heights, weights, test scores under normal conditions). It’s useful in many classical statistical methods and hypothesis testing.
  
- **Smoothened KDE Plot**: Use this when you want to explore the underlying distribution of a dataset without assuming a specific form. It's particularly helpful in exploratory data analysis and when you suspect the data might not be normally distributed.

### **Conclusion**:
- **Gaussian Distribution** is a theoretical model with a fixed, bell-shaped curve that works well for normally distributed data.
- **KDE Plot** is more flexible, as it is based directly on the data and can show a variety of distributions, from unimodal to multi-modal shapes. It is ideal for estimating the probability density without assuming a specific distribution like the Gaussian.

(ii) Write a Python function to calculate the range of a given list of numbers.
"""

def calculate_range(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Calculate the range (max - min)
    range_value = max(numbers) - min(numbers)

    return range_value

# Example usage:
numbers = [5, 10, 2, 8, 7]
print("Range:", calculate_range(numbers))

""" (iii) Create a program to find the variance and standard deviation of a list of numbers."""

import math

def calculate_variance_and_std(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Calculate the mean
    mean = sum(numbers) / len(numbers)

    # Calculate the variance
    squared_diffs = [(x - mean) ** 2 for x in numbers]
    variance = sum(squared_diffs) / len(numbers)

    # Calculate the standard deviation
    std_deviation = math.sqrt(variance)

    return variance, std_deviation

# Example usage:
numbers = [5, 10, 2, 8, 7]
variance, std_deviation = calculate_variance_and_std(numbers)

print("Variance:", variance)
print("Standard Deviation:", std_deviation)

""" (iv) Implement a function to compute the interquartile range (IQR) of a list of values."""

import numpy as np

def calculate_iqr(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Sort the list of numbers
    numbers.sort()

    # Calculate the first quartile (Q1) and third quartile (Q3)
    Q1 = np.percentile(numbers, 25)
    Q3 = np.percentile(numbers, 75)

    # Calculate the interquartile range (IQR)
    IQR = Q3 - Q1

    return IQR

# Example usage:
numbers = [5, 10, 2, 8, 7, 12, 15, 14, 9]
IQR = calculate_iqr(numbers)

print("Interquartile Range (IQR):", IQR)

"""(v) Build a program to calculate the coefficient of variation for a dataset"""

import math

def calculate_cv(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Calculate the mean
    mean = sum(numbers) / len(numbers)

    # Calculate the variance
    squared_diffs = [(x - mean) ** 2 for x in numbers]
    variance = sum(squared_diffs) / len(numbers)

    # Calculate the standard deviation
    std_deviation = math.sqrt(variance)

    # Calculate the coefficient of variation
    cv = (std_deviation / mean) * 100

    return cv

# Example usage:
numbers = [5, 10, 2, 8, 7]
cv = calculate_cv(numbers)

print(f"Coefficient of Variation (CV): {cv:.2f}%")

""" (vi) Write a Python function to find the mean absolute deviation (MAD) of a list of numbers"""

def calculate_mad(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Calculate the mean
    mean = sum(numbers) / len(numbers)

    # Calculate the absolute deviations from the mean
    absolute_deviations = [abs(x - mean) for x in numbers]

    # Calculate the mean absolute deviation (MAD)
    mad = sum(absolute_deviations) / len(numbers)

    return mad

# Example usage:
numbers = [5, 10, 2, 8, 7]
mad = calculate_mad(numbers)

print(f"Mean Absolute Deviation (MAD): {mad:.2f}")

"""(vii) Create a program to calculate the quartile deviation of a list of values."""

import numpy as np

def calculate_quartile_deviation(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Sort the list of numbers
    numbers.sort()

    # Calculate the first quartile (Q1) and third quartile (Q3)
    Q1 = np.percentile(numbers, 25)
    Q3 = np.percentile(numbers, 75)

    # Calculate the quartile deviation (QD)
    QD = (Q3 - Q1) / 2

    return QD

# Example usage:
numbers = [5, 10, 2, 8, 7, 12, 15, 14, 9]
QD = calculate_quartile_deviation(numbers)

print(f"Quartile Deviation (QD): {QD:.2f}")

"""(viii) Implement a function to find the range-based coefficient of dispersion for a dataset."""

def calculate_cod(numbers):
    # Check if the list is not empty
    if len(numbers) == 0:
        return "The list is empty."

    # Calculate the mean
    mean = sum(numbers) / len(numbers)

    # Calculate the range (max - min)
    range_value = max(numbers) - min(numbers)

    # Calculate the Coefficient of Dispersion (COD)
    cod = range_value / mean

    return cod

# Example usage:
numbers = [5, 10, 2, 8, 7]
cod = calculate_cod(numbers)

print(f"Range-Based Coefficient of Dispersion (COD): {cod:.2f}")

"""**Q3.Write a Python class representing a discrete random variable with methods to calculate its expected**
**value and variance.**
"""

class DiscreteRandomVariable:
    def __init__(self, values, probabilities):
        # Ensure the values and probabilities are valid
        if len(values) != len(probabilities):
            raise ValueError("Values and probabilities must have the same length.")
        if not (0 < sum(probabilities) <= 1):
            raise ValueError("The sum of probabilities must be between 0 and 1.")
        if any(p < 0 or p > 1 for p in probabilities):
            raise ValueError("Probabilities must be between 0 and 1.")

        self.values = values
        self.probabilities = probabilities

    def expected_value(self):
        """Calculate the expected value (mean) of the random variable."""
        return sum(x * p for x, p in zip(self.values, self.probabilities))

    def variance(self):
        """Calculate the variance of the random variable."""
        # E[X^2]
        expected_value_squared = sum((x ** 2) * p for x, p in zip(self.values, self.probabilities))
        # Variance = E[X^2] - (E[X])^2
        return expected_value_squared - self.expected_value() ** 2

# Example usage:
values = [1, 2, 3, 4]
probabilities = [0.1, 0.2, 0.3, 0.4]

random_var = DiscreteRandomVariable(values, probabilities)

print(f"Expected Value: {random_var.expected_value()}")
print(f"Variance: {random_var.variance()}")

"""**Q4.Implement a program to simulate the rolling of a fair six-sided die and calculate the expected value and**
**variance of the outcomes.**
"""

import numpy as np

def simulate_die_rolls(num_rolls=1000):
    """
    Simulate the rolling of a fair six-sided die for a given number of rolls.
    It calculates and returns the expected value and variance of the outcomes.

    :param num_rolls: Number of rolls to simulate. Default is 1000.
    :return: A dictionary with the expected value and variance of the outcomes.
    """
    # Simulate the die rolls: 1 to 6 outcomes, each with a probability of 1/6
    rolls = np.random.choice([1, 2, 3, 4, 5, 6], size=num_rolls, p=[1/6] * 6)

    # Calculate the expected value (mean) of the rolls
    expected_value = np.mean(rolls)

    # Calculate the variance of the rolls
    variance = np.var(rolls)

    return {'expected_value': expected_value, 'variance': variance}

# Example usage:
results = simulate_die_rolls(1000)

print(f"Expected Value: {results['expected_value']}")
print(f"Variance: {results['variance']}")

"""**Q5.Create a Python function to generate random samples from a given probability distribution (e.g.,**
**binomial, Poisson) and calculate their mean and variance.**
"""

import numpy as np

def generate_random_samples(distribution, **params):
    """
    Generate random samples from a given probability distribution and calculate mean and variance.

    :param distribution: The type of distribution ('binomial' or 'poisson').
    :param params: Parameters for the distribution.
                   For 'binomial', provide 'n' (number of trials) and 'p' (probability of success).
                   For 'poisson', provide 'lambda_' (rate of occurrence). # Changed to lambda_
    :return: A dictionary with 'samples', 'mean', and 'variance' of the generated samples.
    """

    if distribution == 'binomial':
        n = params.get('n')  # number of trials
        p = params.get('p')  # probability of success
        if n is None or p is None:
            raise ValueError("For binomial distribution, 'n' and 'p' must be provided.")

        # Generate random samples from binomial distribution
        samples = np.random.binomial(n, p, size=1000)

        # Calculate mean and variance of the samples
        mean = np.mean(samples)
        variance = np.var(samples)

    elif distribution == 'poisson':
        lambda_ = params.get('lambda_')  # rate of occurrence # Changed from lambda to lambda_
        if lambda_ is None:
            raise ValueError("For Poisson distribution, 'lambda_' must be provided.") # Changed to lambda_

        # Generate random samples from Poisson distribution
        samples = np.random.poisson(lambda_, size=1000)

        # Calculate mean and variance of the samples
        mean = np.mean(samples)
        variance = np.var(samples)

    else:
        raise ValueError("Unsupported distribution type. Use 'binomial' or 'poisson'.")

    return {'samples': samples, 'mean': mean, 'variance': variance}

# Example usage:
# For binomial distribution with n=10 trials and p=0.5 probability
binomial_results = generate_random_samples('binomial', n=10, p=0.5)
print("Binomial Distribution:")
print(f"Mean: {binomial_results['mean']}")
print(f"Variance: {binomial_results['variance']}")

# For Poisson distribution with lambda=3 rate of occurrence
poisson_results = generate_random_samples('poisson', lambda_=3) # Changed to lambda_
print("\nPoisson Distribution:")
print(f"Mean: {poisson_results['mean']}")
print(f"Variance: {poisson_results['variance']}")

"""**Q6.Write a Python script to generate random numbers from a Gaussian (normal) distribution and compute**
**the mean, variance, and standard deviation of the samples**.
"""

import numpy as np

def generate_gaussian_samples(mean=0, std_dev=1, num_samples=1000):
    """
    Generate random samples from a Gaussian (normal) distribution and calculate mean, variance, and standard deviation.

    :param mean: The mean (μ) of the normal distribution (default is 0).
    :param std_dev: The standard deviation (σ) of the normal distribution (default is 1).
    :param num_samples: The number of samples to generate (default is 1000).
    :return: A dictionary with 'mean', 'variance', and 'standard_deviation' of the samples.
    """
    # Generate random samples from the normal distribution
    samples = np.random.normal(loc=mean, scale=std_dev, size=num_samples)

    # Calculate the mean, variance, and standard deviation of the samples
    calculated_mean = np.mean(samples)
    calculated_variance = np.var(samples)
    calculated_std_dev = np.std(samples)

    return {
        'mean': calculated_mean,
        'variance': calculated_variance,
        'standard_deviation': calculated_std_dev
    }

# Example usage:
# Generate samples from a normal distribution with mean=0 and standard deviation=1
results = generate_gaussian_samples(mean=0, std_dev=1, num_samples=1000)

print(f"Mean of samples: {results['mean']}")
print(f"Variance of samples: {results['variance']}")
print(f"Standard Deviation of samples: {results['standard_deviation']}")

"""**Q7.Use seaborn library to load tips dataset. Find the following from the dataset for the columns total_bill and tip`:**"""

import seaborn as sns
import numpy as np

# Load the tips dataset from seaborn
tips = sns.load_dataset('tips')

# Extract the 'total_bill' and 'tip' columns
total_bill = tips['total_bill']
tip = tips['tip']

# Compute mean, variance, and standard deviation for 'total_bill' and 'tip'
total_bill_mean = np.mean(total_bill)
total_bill_variance = np.var(total_bill)
total_bill_std_dev = np.std(total_bill)

tip_mean = np.mean(tip)
tip_variance = np.var(tip)
tip_std_dev = np.std(tip)

# Display the results
print(f"Total Bill - Mean: {total_bill_mean}, Variance: {total_bill_variance}, Standard Deviation: {total_bill_std_dev}")
print(f"Tip - Mean: {tip_mean}, Variance: {tip_variance}, Standard Deviation: {tip_std_dev}")

""" (i) Write a Python function that calculates their skewness."""

import seaborn as sns
from scipy.stats import skew

def calculate_skewness(dataset, column):
    """
    Calculate the skewness of a given column in a dataset.

    :param dataset: pandas DataFrame containing the dataset.
    :param column: The column name (string) whose skewness is to be calculated.
    :return: Skewness value for the specified column.
    """
    data = dataset[column]
    return skew(data)

# Load the tips dataset from seaborn
tips = sns.load_dataset('tips')

# Calculate skewness for 'total_bill' and 'tip' columns
total_bill_skewness = calculate_skewness(tips, 'total_bill')
tip_skewness = calculate_skewness(tips, 'tip')

# Display the results
print(f"Skewness of total_bill: {total_bill_skewness}")
print(f"Skewness of tip: {tip_skewness}")

""" (ii) Create a program that determines whether the columns exhibit positive skewness, negative skewness, or is
approximately symmetric.
"""

import seaborn as sns
from scipy.stats import skew

def calculate_skewness(dataset, column):
    """
    Calculate the skewness of a given column in a dataset.

    :param dataset: pandas DataFrame containing the dataset.
    :param column: The column name (string) whose skewness is to be calculated.
    :return: Skewness value for the specified column.
    """
    data = dataset[column]
    return skew(data)

def determine_skewness_type(skewness_value):
    """
    Determine the skewness type based on the skewness value.

    :param skewness_value: The skewness value of the dataset column.
    :return: A string indicating whether the distribution is 'Positive Skew', 'Negative Skew', or 'Approximately Symmetric'.
    """
    if skewness_value > 0.5:
        return 'Positive Skew'
    elif skewness_value < -0.5:
        return 'Negative Skew'
    else:
        return 'Approximately Symmetric'

# Load the tips dataset from seaborn
tips = sns.load_dataset('tips')

# Calculate skewness for 'total_bill' and 'tip' columns
total_bill_skewness = calculate_skewness(tips, 'total_bill')
tip_skewness = calculate_skewness(tips, 'tip')

# Determine skewness type for 'total_bill' and 'tip' columns
total_bill_skewness_type = determine_skewness_type(total_bill_skewness)
tip_skewness_type = determine_skewness_type(tip_skewness)

# Display the results
print(f"Skewness of total_bill: {total_bill_skewness} - {total_bill_skewness_type}")
print(f"Skewness of tip: {tip_skewness} - {tip_skewness_type}")

""" (iii) Write a function that calculates the covariance between two columns."""

import numpy as np

def calculate_covariance(column1, column2):
    """
    Calculate the covariance between two columns of data.

    :param column1: List or numpy array containing data for the first column.
    :param column2: List or numpy array containing data for the second column.
    :return: Covariance between the two columns.
    """
    # Convert inputs to numpy arrays
    column1 = np.array(column1)
    column2 = np.array(column2)

    # Calculate the means of the two columns
    mean_column1 = np.mean(column1)
    mean_column2 = np.mean(column2)

    # Compute the covariance using the formula
    covariance = np.sum((column1 - mean_column1) * (column2 - mean_column2)) / (len(column1) - 1)

    return covariance

# Example usage
column1 = [10, 20, 30, 40, 50]  # Example data for first column
column2 = [1, 2, 3, 4, 5]  # Example data for second column

cov = calculate_covariance(column1, column2)
print(f"Covariance between the two columns: {cov}")

""" (iv) Implement a Python program that calculates the Pearson correlation coefficient between two columns."""

import numpy as np

def calculate_pearson_correlation(column1, column2):
    """
    Calculate the Pearson correlation coefficient between two columns of data.

    :param column1: List or numpy array containing data for the first column.
    :param column2: List or numpy array containing data for the second column.
    :return: Pearson correlation coefficient between the two columns.
    """
    # Convert inputs to numpy arrays
    column1 = np.array(column1)
    column2 = np.array(column2)

    # Calculate the covariance between the two columns
    covariance = np.sum((column1 - np.mean(column1)) * (column2 - np.mean(column2))) / (len(column1) - 1)

    # Calculate the standard deviations of the two columns
    std_dev_column1 = np.std(column1, ddof=1)
    std_dev_column2 = np.std(column2, ddof=1)

    # Calculate the Pearson correlation coefficient
    pearson_correlation = covariance / (std_dev_column1 * std_dev_column2)

    return pearson_correlation

# Example usage
column1 = [10, 20, 30, 40, 50]  # Example data for the first column
column2 = [1, 2, 3, 4, 5]  # Example data for the second column

# Calculate Pearson correlation
correlation = calculate_pearson_correlation(column1, column2)
print(f"Pearson correlation coefficient between the two columns: {correlation}")

""" (v) Write a script to visualize the correlation between two specific columns in a Pandas DataFrame using scatter plots."""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def visualize_correlation(df, column1, column2):
    """
    Visualize the correlation between two specific columns in a DataFrame using a scatter plot.

    :param df: Pandas DataFrame containing the data.
    :param column1: Name of the first column (string).
    :param column2: Name of the second column (string).
    """
    # Create a scatter plot using seaborn
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=df[column1], y=df[column2])

    # Add labels and title
    plt.title(f"Scatter Plot: {column1} vs {column2}", fontsize=16)
    plt.xlabel(column1, fontsize=12)
    plt.ylabel(column2, fontsize=12)

    # Show the plot
    plt.show()

# Example usage
# Load a sample dataset (e.g., tips dataset from seaborn)
import seaborn as sns
df = sns.load_dataset('tips')

# Visualize correlation between 'total_bill' and 'tip' columns
visualize_correlation(df, 'total_bill', 'tip')

"""**Q8.Write a Python function to calculate the probability density function (PDF) of a continuous random**
**variable for a given normal distribution.**
"""

import numpy as np
from scipy.stats import norm

def normal_pdf(x, mu, sigma):
    """
    Calculate the Probability Density Function (PDF) for a normal distribution.

    :param x: The value(s) for which to calculate the PDF (single value or array).
    :param mu: The mean of the normal distribution.
    :param sigma: The standard deviation of the normal distribution.
    :return: The PDF value(s) for the input value(s).
    """
    # Calculate PDF using the normal distribution formula
    pdf_value = (1 / (np.sqrt(2 * np.pi * sigma**2))) * np.exp(-0.5 * ((x - mu) / sigma)**2)

    return pdf_value

# Example usage:
mu = 0     # Mean of the normal distribution
sigma = 1  # Standard deviation of the normal distribution

# Calculate PDF for a single value
x = 1
pdf_single = normal_pdf(x, mu, sigma)
print(f"PDF at x = {x}: {pdf_single}")

# Calculate PDF for multiple values (e.g., from -3 to 3)
x_values = np.linspace(-3, 3, 100)
pdf_values = normal_pdf(x_values, mu, sigma)

# Plotting the PDF (optional visualization)
import matplotlib.pyplot as plt
plt.plot(x_values, pdf_values)
plt.title('Normal Distribution PDF')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.grid(True)
plt.show()

from scipy.stats import norm

# Calculate PDF using scipy's norm.pdf
pdf_value_scipy = norm.pdf(x, mu, sigma)
print(f"PDF using scipy at x = {x}: {pdf_value_scipy}")

"""**Q9.Create a program to calculate the cumulative distribution function (CDF) of exponential distribution.**"""

import numpy as np

def exponential_cdf(x, rate):
    """
    Calculate the Cumulative Distribution Function (CDF) of an exponential distribution.

    :param x: The value(s) for which to calculate the CDF (single value or array).
    :param rate: The rate parameter (lambda) of the exponential distribution.
    :return: The CDF value(s) for the input value(s).
    """
    # Ensure that x is a numpy array for vectorized operations
    x = np.array(x)

    # Calculate the CDF for exponential distribution
    cdf_value = 1 - np.exp(-rate * x)

    return cdf_value

# Example usage:
rate = 1.0  # Rate parameter (lambda) for the exponential distribution
x_values = np.linspace(0, 5, 100)  # Generate 100 values from 0 to 5

# Calculate CDF for these x values
cdf_values = exponential_cdf(x_values, rate)

# Plotting the CDF (optional visualization)
import matplotlib.pyplot as plt

plt.plot(x_values, cdf_values)
plt.title('CDF of Exponential Distribution')
plt.xlabel('x')
plt.ylabel('CDF')
plt.grid(True)
plt.show()

"""**Q10.Write a Python function to calculate the probability mass function (PMF) of Poisson distribution.**"""

import math

def poisson_pmf(k, rate):
    """
    Calculate the Probability Mass Function (PMF) of a Poisson distribution.

    :param k: The number of occurrences (non-negative integer).
    :param rate: The rate parameter (lambda) of the Poisson distribution.
    :return: The PMF value for the input value(s).
    """
    if k < 0 or not isinstance(k, int):
        raise ValueError("k must be a non-negative integer.")

    # Calculate PMF using the Poisson distribution formula
    pmf_value = (rate ** k * math.exp(-rate)) / math.factorial(k)

    return pmf_value

# Example usage:
rate = 3  # The rate parameter (lambda)
k = 4     # The number of occurrences (k)

# Calculate PMF for the given k and rate
pmf_value = poisson_pmf(k, rate)
print(f"PMF of Poisson distribution for k = {k} and rate = {rate}: {pmf_value}")

"""**Q11.A company wants to test if a new website layout leads to a higher conversion rate (percentage of visitors**
**who make a purchase). They collect data from the old and new layouts to compare.**

To generate the data use the following command:

```python

import numpy as np

# 50 purchases out of 1000 visitors

old_layout = np.array([1] * 50 + [0] * 950)

# 70 purchases out of 1000 visitors  

new_layout = np.array([1] * 70 + [0] * 930)

  ```

Apply z-test to find which layout is successful.

"""

import numpy as np
import math
from scipy.stats import norm

# Data for the old and new layouts
old_layout = np.array([1] * 50 + [0] * 950)
new_layout = np.array([1] * 70 + [0] * 930)

# Number of visitors (n) and successes (x) for both old and new layouts
n_old = len(old_layout)
n_new = len(new_layout)
x_old = np.sum(old_layout)
x_new = np.sum(new_layout)

# Sample proportions
p_old = x_old / n_old
p_new = x_new / n_new

# Pooled proportion
p_pool = (x_old + x_new) / (n_old + n_new)

# Standard error of the difference
SE = math.sqrt(p_pool * (1 - p_pool) * (1/n_old + 1/n_new))

# Z-statistic
z = (p_new - p_old) / SE

# P-value (one-tailed test)
p_value = 1 - norm.cdf(z)

# Output results
print(f"z-statistic: {z}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The new layout significantly increases the conversion rate.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in conversion rates.")

"""Q12.A tutoring service claims that its program improves students' exam scores. A sample of students who
participated in the program was taken, and their scores before and after the program were recorded.

Use the below code to generate samples of respective arrays of marks:

```python

before_program = np.array([75, 80, 85, 70, 90, 78, 92, 88, 82, 87])

after_program = np.array([80, 85, 90, 80, 92, 80, 95, 90, 85, 88])

```

Use z-test to find if the claims made by tutor are true or false.
"""

import numpy as np
import math
from scipy.stats import norm

# Data for the students' scores before and after the program
before_program = np.array([75, 80, 85, 70, 90, 78, 92, 88, 82, 87])
after_program = np.array([80, 85, 90, 80, 92, 80, 95, 90, 85, 88])

# Step 1: Calculate the differences between the before and after scores
differences = after_program - before_program

# Step 2: Calculate the mean and standard deviation of the differences
mean_diff = np.mean(differences)
std_diff = np.std(differences, ddof=1)  # Sample standard deviation

# Step 3: Calculate the standard error of the mean difference
n = len(differences)
SE = std_diff / math.sqrt(n)

# Step 4: Calculate the z-statistic
z = mean_diff / SE

# Step 5: Calculate the p-value for a one-tailed test
p_value = 1 - norm.cdf(z)

# Output results
print(f"Mean of differences: {mean_diff}")
print(f"Standard deviation of differences: {std_diff}")
print(f"Standard error: {SE}")
print(f"z-statistic: {z}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The tutoring program significantly improves exam scores.")
else:
    print("Fail to reject the null hypothesis: There is no significant improvement in exam scores.")

"""Q13. A pharmaceutical company wants to determine if a new drug is effective in reducing blood pressure. They
conduct a study and record blood pressure measurements before and after administering the drug.

Use the below code to generate samples of respective arrays of blood pressure:


```python

before_drug = np.array([145, 150, 140, 135, 155, 160, 152, 148, 130, 138])

after_drug = np.array([130, 140, 132, 128, 145, 148, 138, 136, 125, 130])

  ```


Implement z-test to find if the drug really works or not
"""

import numpy as np
import math
from scipy.stats import norm

# Data for blood pressure before and after the drug
before_drug = np.array([145, 150, 140, 135, 155, 160, 152, 148, 130, 138])
after_drug = np.array([130, 140, 132, 128, 145, 148, 138, 136, 125, 130])

# Step 1: Calculate the differences between before and after the drug
differences = before_drug - after_drug

# Step 2: Calculate the mean and standard deviation of the differences
mean_diff = np.mean(differences)
std_diff = np.std(differences, ddof=1)  # Sample standard deviation

# Step 3: Calculate the standard error of the mean difference
n = len(differences)
SE = std_diff / math.sqrt(n)

# Step 4: Calculate the z-statistic
z = mean_diff / SE

# Step 5: Calculate the p-value for a one-tailed test
p_value = norm.cdf(z)

# Output results
print(f"Mean of differences: {mean_diff}")
print(f"Standard deviation of differences: {std_diff}")
print(f"Standard error: {SE}")
print(f"z-statistic: {z}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The drug significantly reduces blood pressure.")
else:
    print("Fail to reject the null hypothesis: There is no significant reduction in blood pressure.")

"""**Q14.A customer service department claims that their average response time is less than 5 minutes. A sample**
**of recent customer interactions was taken, and the response times were recorded**.

Implement the below code to generate the array of response time:

```python

response_times = np.array([4.3, 3.8, 5.1, 4.9, 4.7, 4.2, 5.2, 4.5, 4.6, 4.4])

```

Implement z-test to find the claims made by customer service department are tru or false.
"""

import numpy as np
import math
from scipy.stats import norm

# Data for response times
response_times = np.array([4.3, 3.8, 5.1, 4.9, 4.7, 4.2, 5.2, 4.5, 4.6, 4.4])

# Step 1: Calculate the sample mean and sample standard deviation
sample_mean = np.mean(response_times)
sample_std = np.std(response_times, ddof=1)  # Sample standard deviation

# Step 2: Calculate the standard error of the mean
n = len(response_times)
SE = sample_std / math.sqrt(n)

# Step 3: Calculate the z-statistic
population_mean = 5  # Claimed average response time
z = (sample_mean - population_mean) / SE

# Step 4: Calculate the p-value for a one-tailed test
p_value = norm.cdf(z)

# Output results
print(f"Sample mean: {sample_mean}")
print(f"Sample standard deviation: {sample_std}")
print(f"Standard error: {SE}")
print(f"z-statistic: {z}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The response time is significantly less than 5 minutes.")
else:
    print("Fail to reject the null hypothesis: The response time is not significantly less than 5 minutes.")

"""**Q15.A company is testing two different website layouts to see which one leads to higher click-through rates.**
**Write a Python function to perform an A/B test analysis, including calculating the t-statistic, degrees of**
**freedom, and p-value.**

Use the following data:

```python

layout_a_clicks = [28, 32, 33, 29, 31, 34, 30, 35, 36, 37]

layout_b_clicks = [40, 41, 38, 42, 39, 44, 43, 41, 45, 47]
"""

import numpy as np
import math
from scipy import stats

# Data for click-through rates
layout_a_clicks = [28, 32, 33, 29, 31, 34, 30, 35, 36, 37]
layout_b_clicks = [40, 41, 38, 42, 39, 44, 43, 41, 45, 47]

# Calculate sample means and standard deviations
mean_a = np.mean(layout_a_clicks)
mean_b = np.mean(layout_b_clicks)
std_a = np.std(layout_a_clicks, ddof=1)  # Sample standard deviation
std_b = np.std(layout_b_clicks, ddof=1)

# Sample sizes
n_a = len(layout_a_clicks)
n_b = len(layout_b_clicks)

# Calculate the t-statistic
t_stat = (mean_a - mean_b) / math.sqrt((std_a**2 / n_a) + (std_b**2 / n_b))

# Calculate degrees of freedom (Welch-Satterthwaite equation)
df = ((std_a**2 / n_a + std_b**2 / n_b)**2) / ((std_a**2 / n_a)**2 / (n_a - 1) + (std_b**2 / n_b)**2 / (n_b - 1))

# Calculate the p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)  # Two-tailed test

# Output results
print(f"Mean of layout A: {mean_a}")
print(f"Mean of layout B: {mean_b}")
print(f"Standard deviation of layout A: {std_a}")
print(f"Standard deviation of layout B: {std_b}")
print(f"t-statistic: {t_stat}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference between the two layouts.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference between the two layouts.")

"""**Q16.A pharmaceutical company wants to determine if a new drug is more effective than an existing drug in**
**reducing cholesterol levels. Create a program to analyze the clinical trial data and calculate the tstatistic and p-value for the treatment effect.**

Use the following data of cholestrol level:

```python

existing_drug_levels = [180, 182, 175, 185, 178, 176, 172, 184, 179, 183]

new_drug_levels = [170, 172, 165, 168, 175, 173, 170, 178, 172, 176]
"""

import numpy as np
import math
from scipy import stats

# Data for cholesterol levels
existing_drug_levels = [180, 182, 175, 185, 178, 176, 172, 184, 179, 183]
new_drug_levels = [170, 172, 165, 168, 175, 173, 170, 178, 172, 176]

# Calculate sample means and standard deviations
mean_existing = np.mean(existing_drug_levels)
mean_new = np.mean(new_drug_levels)
std_existing = np.std(existing_drug_levels, ddof=1)  # Sample standard deviation
std_new = np.std(new_drug_levels, ddof=1)

# Sample sizes
n_existing = len(existing_drug_levels)
n_new = len(new_drug_levels)

# Calculate the t-statistic
t_stat = (mean_existing - mean_new) / math.sqrt((std_existing**2 / n_existing) + (std_new**2 / n_new))

# Calculate degrees of freedom (Welch-Satterthwaite equation)
df = ((std_existing**2 / n_existing + std_new**2 / n_new)**2) / ((std_existing**2 / n_existing)**2 / (n_existing - 1) + (std_new**2 / n_new)**2 / (n_new - 1))

# Calculate the p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)  # Two-tailed test

# Output results

"""**Q17.A school district introduces an educational intervention program to improve math scores. Write a Python**
**function to analyze pre- and post-intervention test scores, calculating the t-statistic and p-value to**
**determine if the intervention had a significant impact.**

Use the following data of test score:


  ```python

  pre_intervention_scores = [80, 85, 90, 75, 88, 82, 92, 78, 85, 87]

  post_intervention_scores = [90, 92, 88, 92, 95, 91, 96, 93, 89, 93]
"""

import numpy as np
import math
from scipy import stats

# Data for test scores before and after intervention
pre_intervention_scores = [80, 85, 90, 75, 88, 82, 92, 78, 85, 87]
post_intervention_scores = [90, 92, 88, 92, 95, 91, 96, 93, 89, 93]

# Calculate the differences (post - pre)
differences = np.array(post_intervention_scores) - np.array(pre_intervention_scores)

# Calculate the mean and standard deviation of the differences
mean_difference = np.mean(differences)
std_difference = np.std(differences, ddof=1)  # Sample standard deviation
n = len(differences)

# Calculate the t-statistic
t_stat = mean_difference / (std_difference / math.sqrt(n))

# Calculate the degrees of freedom
df = n - 1

# Calculate the p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)  # Two-tailed test

# Output results
print(f"Mean of differences: {mean_difference}")
print(f"Standard deviation of differences: {std_difference}")
print(f"t-statistic: {t_stat}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The intervention had a significant effect on test scores.")
else:
    print("Fail to reject the null hypothesis: There is no significant effect of the intervention on test scores.")

"""**Q18.An HR department wants to investigate if there's a gender-based salary gap within the company. Develop**
**a program to analyze salary data, calculate the t-statistic, and determine if there's a statistically**
**significant difference between the average salaries of male and female employees.**

Use the below code to generate synthetic data:


```python

# Generate synthetic salary data for male and female employees

np.random.seed(0)  # For reproducibility

male_salaries = np.random.normal(loc=50000, scale=10000, size=20)

female_salaries = np.random.normal(loc=55000, scale=9000, size=20)

  ```

"""

import numpy as np
import math
from scipy import stats

# Generate synthetic salary data
np.random.seed(0)
male_salaries = np.random.normal(loc=50000, scale=10000, size=20)
female_salaries = np.random.normal(loc=55000, scale=9000, size=20)

# Calculate sample means and standard deviations
mean_male = np.mean(male_salaries)
mean_female = np.mean(female_salaries)
std_male = np.std(male_salaries, ddof=1)  # Sample standard deviation
std_female = np.std(female_salaries, ddof=1)

# Sample sizes
n_male = len(male_salaries)
n_female = len(female_salaries)

# Calculate the t-statistic
t_stat = (mean_male - mean_female) / math.sqrt((std_male**2 / n_male) + (std_female**2 / n_female))

# Calculate degrees of freedom (Welch-Satterthwaite equation)
df = ((std_male**2 / n_male + std_female**2 / n_female)**2) / ((std_male**2 / n_male)**2 / (n_male - 1) + (std_female**2 / n_female)**2 / (n_female - 1))

# Calculate the p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)  # Two-tailed test

# Output results
print(f"Mean salary (male): {mean_male}")
print(f"Mean salary (female): {mean_female}")
print(f"Standard deviation (male): {std_male}")
print(f"Standard deviation (female): {std_female}")
print(f"t-statistic: {t_stat}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a statistically significant gender-based salary gap.")
else:
    print("Fail to reject the null hypothesis: There is no statistically significant gender-based salary gap.")

"""**Q19.A manufacturer produces two different versions of a product and wants to compare their quality scores.**
**Create a Python function to analyze quality assessment data, calculate the t-statistic, and decide**
**whether there's a significant difference in quality between the two versions**

Use the following data:


```python

version1_scores = [85, 88, 82, 89, 87, 84, 90, 88, 85, 86, 91, 83, 87, 84, 89, 86, 84, 88, 85, 86, 89, 90, 87, 88, 85]

version2_scores = [80, 78, 83, 81, 79, 82, 76, 80, 78, 81, 77, 82, 80, 79, 82, 79, 80, 81, 79, 82, 79, 78, 80, 81, 82]
"""

import numpy as np
from scipy import stats
import math

# Quality scores for each version
version1_scores = [85, 88, 82, 89, 87, 84, 90, 88, 85, 86, 91, 83, 87, 84, 89, 86, 84, 88, 85, 86, 89, 90, 87, 88, 85]
version2_scores = [80, 78, 83, 81, 79, 82, 76, 80, 78, 81, 77, 82, 80, 79, 82, 79, 80, 81, 79, 82, 79, 78, 80, 81, 82]

# Calculate means and standard deviations
mean_version1 = np.mean(version1_scores)
mean_version2 = np.mean(version2_scores)
std_version1 = np.std(version1_scores, ddof=1)  # Sample standard deviation
std_version2 = np.std(version2_scores, ddof=1)

# Sample sizes
n_version1 = len(version1_scores)
n_version2 = len(version2_scores)

# Calculate the t-statistic
t_stat = (mean_version1 - mean_version2) / math.sqrt((std_version1**2 / n_version1) + (std_version2**2 / n_version2))

# Calculate degrees of freedom
df = ((std_version1**2 / n_version1 + std_version2**2 / n_version2)**2) / ((std_version1**2 / n_version1)**2 / (n_version1 - 1) + (std_version2**2 / n_version2)**2 / (n_version2 - 1))

# Calculate the p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)

# Output results
print(f"Mean score (Version 1): {mean_version1}")
print(f"Mean score (Version 2): {mean_version2}")
print(f"Standard deviation (Version 1): {std_version1}")
print(f"Standard deviation (Version 2): {std_version2}")
print(f"t-statistic: {t_stat}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a statistically significant difference in quality between the two versions.")
else:
    print("Fail to reject the null hypothesis: There is no statistically significant difference in quality between the two versions.")

"""**Q20.A restaurant chain collects customer satisfaction scores for two different branches. Write a program to**
**analyze the scores, calculate the t-statistic, and determine if there's a statistically significant difference in**
**customer satisfaction between the branches.**

Use the below data of scores:

  ```python

branch_a_scores = [4, 5, 3, 4, 5, 4, 5, 3, 4, 4, 5, 4, 4, 3, 4, 5, 5, 4, 3, 4, 5, 4, 3, 5, 4, 4, 5, 3, 4, 5, 4]

branch_b_scores = [3, 4, 2, 3, 4, 3, 4, 2, 3, 3, 4, 3, 3, 2, 3, 4, 4, 3, 2, 3, 4, 3, 2, 4, 3, 3, 4, 2, 3, 4, 3]
"""

import numpy as np
from scipy import stats
import math

# Customer satisfaction scores for each branch
branch_a_scores = [4, 5, 3, 4, 5, 4, 5, 3, 4, 4, 5, 4, 4, 3, 4, 5, 5, 4, 3, 4, 5, 4, 3, 5, 4, 4, 5, 3, 4, 5, 4]
branch_b_scores = [3, 4, 2, 3, 4, 3, 4, 2, 3, 3, 4, 3, 3, 2, 3, 4, 4, 3, 2, 3, 4, 3, 2, 4, 3, 3, 4, 2, 3, 4, 3]

# Calculate means and standard deviations
mean_branch_a = np.mean(branch_a_scores)
mean_branch_b = np.mean(branch_b_scores)
std_branch_a = np.std(branch_a_scores, ddof=1)  # Sample standard deviation
std_branch_b = np.std(branch_b_scores, ddof=1)

# Sample sizes
n_branch_a = len(branch_a_scores)
n_branch_b = len(branch_b_scores)

# Calculate the t-statistic
t_stat = (mean_branch_a - mean_branch_b) / math.sqrt((std_branch_a**2 / n_branch_a) + (std_branch_b**2 / n_branch_b))

# Calculate degrees of freedom
df = ((std_branch_a**2 / n_branch_a + std_branch_b**2 / n_branch_b)**2) / ((std_branch_a**2 / n_branch_a)**2 / (n_branch_a - 1) + (std_branch_b**2 / n_branch_b)**2 / (n_branch_b - 1))

# Calculate p-value (two-tailed test)
p_value = 2 * stats.t.cdf(-abs(t_stat), df)

# Output results
print(f"Mean score (Branch A): {mean_branch_a}")
print(f"Mean score (Branch B): {mean_branch_b}")
print(f"Standard deviation (Branch A): {std_branch_a}")
print(f"Standard deviation (Branch B): {std_branch_b}")
print(f"t-statistic: {t_stat}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value}")

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a statistically significant difference in customer satisfaction between the branches.")
else:
    print("Fail to reject the null hypothesis: There is no statistically significant difference in customer satisfaction between the branches.")

"""**Q21.A political analyst wants to determine if there is a significant association between age groups and voter**
**preferences (Candidate A or Candidate B). They collect data from a sample of 500 voters and classify**
**them into different age groups and candidate preferences. Perform a Chi-Square test to determine if**
**there is a significant association between age groups and voter preferences.**

Use the below code to generate data:

```python

np.random.seed(0)

age_groups = np.random.choice([ 18 - 30 , 31 - 50 , 51+', 51+'], size=30)

voter_preferences = np.random.choice(['Candidate A', 'Candidate B'], size=30)
"""

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

# Seed for reproducibility
np.random.seed(0)

# Generate data
age_groups = np.random.choice(['18-30', '31-50', '51+'], size=500)
voter_preferences = np.random.choice(['Candidate A', 'Candidate B'], size=500)

# Create a DataFrame
data = pd.DataFrame({
    'Age Group': age_groups,
    'Voter Preference': voter_preferences
})

# Create contingency table
contingency_table = pd.crosstab(data['Age Group'], data['Voter Preference'])

# Perform Chi-Square test
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

# Output results
print("Contingency Table:")
print(contingency_table)
print("\nChi-Square Statistic:", chi2)
print("p-value:", p_value)
print("Degrees of Freedom:", dof)
print("\nExpected Frequencies Table:")
print(pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns))

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("\nReject the null hypothesis: There is a significant association between age groups and voter preferences.")
else:
    print("\nFail to reject the null hypothesis: There is no significant association between age groups and voter preferences.")

"""**Q22. A company conducted a customer satisfaction survey to determine if there is a significant relationship**
**between product satisfaction levels (Satisfied, Neutral, Dissatisfied) and the region where customers are**
**located (East, West, North, South). The survey data is summarized in a contingency table. Conduct a ChiSquare test to determine if there is a** significant relationship between product satisfaction levels and
customer regions.

Sample data:

```python

#Sample data: Product satisfaction levels (rows) vs. Customer regions (columns)

data = np.array([[50, 30, 40, 20], [30, 40, 30, 50], [20, 30, 40, 30]])

```
"""

import numpy as np
from scipy.stats import chi2_contingency

# Sample data: Product satisfaction levels (rows) vs. Customer regions (columns)
data = np.array([
    [50, 30, 40, 20],  # Satisfied
    [30, 40, 30, 50],  # Neutral
    [20, 30, 40, 30]   # Dissatisfied
])

# Perform Chi-Square test
chi2, p_value, dof, expected = chi2_contingency(data)

# Output results
print("Observed Data:")
print(data)
print("\nChi-Square Statistic:", chi2)
print("p-value:", p_value)
print("Degrees of Freedom:", dof)
print("\nExpected Frequencies Table:")
print(np.round(expected, 2))

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("\nReject the null hypothesis: There is a significant relationship between satisfaction levels and customer regions.")
else:
    print("\nFail to reject the null hypothesis: There is no significant relationship between satisfaction levels and customer regions.")

"""**Q23.A company implemented an employee training program to improve job performance (Effective, Neutral,**
I**neffective). After the training, they collected data from a sample of employees and classified them based**
**on their job performance before and after the training. Perform a Chi-Square test to determine if there is a**
**significant difference between job performance levels before and after the training.**

Sample data:

```python

# Sample data: Job performance levels before (rows) and after (columns) training

data = np.array([[50, 30, 20], [30, 40, 30], [20, 30, 40]])

```

"""

import numpy as np
from scipy.stats import chi2_contingency

# Sample data: Job performance levels before (rows) and after (columns) training
data = np.array([
    [50, 30, 20],  # Effective (before)
    [30, 40, 30],  # Neutral (before)
    [20, 30, 40]   # Ineffective (before)
])

# Perform Chi-Square test
chi2, p_value, dof, expected = chi2_contingency(data)

# Output results
print("Observed Data:")
print(data)
print("\nChi-Square Statistic:", chi2)
print("p-value:", p_value)
print("Degrees of Freedom:", dof)
print("\nExpected Frequencies Table:")
print(np.round(expected, 2))

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("\nReject the null hypothesis: There is a significant association between job performance levels before and after training.")
else:
    print("\nFail to reject the null hypothesis: There is no significant association between job performance levels before and after training.")

"""**Q24.A company produces three different versions of a product: Standard, Premium, and Deluxe. The**
**company wants to determine if there is a significant difference in customer satisfaction scores among the**
t**hree product versions. They conducted a survey and collected customer satisfaction scores for each**
**version from a random sample of customers. Perform an ANOVA test to determine if there is a significant**
**difference in customer satisfaction scores.**

 Use the following data:

  ```python

  # Sample data: Customer satisfaction scores for each product version

  standard_scores = [80, 85, 90, 78, 88, 82, 92, 78, 85, 87]

  premium_scores = [90, 92, 88, 92, 95, 91, 96, 93, 89, 93]

  deluxe_scores = [95, 98, 92, 97, 96, 94, 98, 97, 92, 99]

  `
"""

from scipy.stats import f_oneway

# Sample data: Customer satisfaction scores for each product version
standard_scores = [80, 85, 90, 78, 88, 82, 92, 78, 85, 87]
premium_scores = [90, 92, 88, 92, 95, 91, 96, 93, 89, 93]
deluxe_scores = [95, 98, 92, 97, 96, 94, 98, 97, 92, 99]

# Perform one-way ANOVA test
f_statistic, p_value = f_oneway(standard_scores, premium_scores, deluxe_scores)

# Output results
print("F-Statistic:", f_statistic)
print("p-value:", p_value)

# Decision based on p-value
alpha = 0.05
if p_value < alpha:
    print("\nReject the null hypothesis: There is a significant difference in customer satisfaction scores among the product versions.")
else:
    print("\nFail to reject the null hypothesis: There is no significant difference in customer satisfaction scores among the product versions.")